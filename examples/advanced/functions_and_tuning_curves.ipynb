{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nengo\n",
    "from nengo import spa\n",
    "\n",
    "%load_ext nengo.ipynb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving function approximation with adjustment of tuning curves\n",
    "\n",
    "This tutorial shows how adjusting the tuning curves of neurons can help to implement specific functions with Nengo. As an example we try to construct a simple cleanup for semantic pointers: Given an input vector (\"stimulus\"), the network should produce the sum of all semantic pointers which exceed a threshold similarity with the input vector.\n",
    "\n",
    "Before you go through this tutorial, you should be familiar with the basics of Nengo and maybe have some idea of the Semantic Pointer Architecture (SPA).\n",
    "\n",
    "We start by defining our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 64\n",
    "vocab = spa.Vocabulary(d)\n",
    "vocab.parse('A + B + C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input will be $A + 0.2 B$ and we will use a threshold of 0.3. Thus, the cleaned up result should only be $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stimulus_pointer = vocab['A'] + 0.2 * vocab['B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The standard approach\n",
    "\n",
    "As a first pass we just use an ensemble with the default parameters and try to implement the cleanup function with the decoders being solved for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    threshold = 0.3\n",
    "    similarities = np.dot(vocab.vectors, x)\n",
    "    # `thresholded` will be 1 for all semantic pointers in the\n",
    "    # vocabulary exceeding a similarity of 0.3 and 0 otherwise.\n",
    "    thresholded = np.maximum(0, np.sign(similarities - threshold))\n",
    "    # sum all semantic pointers exceeding the threshold\n",
    "    return np.dot(thresholded, vocab.vectors)\n",
    "\n",
    "with nengo.Network() as cleanup:\n",
    "    stimulus = nengo.Node(stimulus_pointer.v)\n",
    "    ens = nengo.Ensemble(n_neurons=150, dimensions=d)\n",
    "    output = nengo.Node(size_in=d)\n",
    "    \n",
    "    nengo.Connection(stimulus, ens)\n",
    "    nengo.Connection(ens, output, function=clean)\n",
    "    \n",
    "    p = nengo.Probe(output, synapse=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with nengo.Simulator(cleanup) as sim:\n",
    "    sim.run(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sim.trange(), spa.similarity(sim.data[p], vocab))\n",
    "plt.legend(vocab.keys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this approach does not work very well. The curves are noisy and even though it is most similar to $A$, this similarity is considerably below 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an ensemble array\n",
    "\n",
    "In the first try we tried to implement the dot product between the input and all vocabulary items with the decoders. We can introduce a transform to do this and then the decoders need to implement only the thresholding. That is a much simpler function.\n",
    "\n",
    "Step by step, the following will happen: The stimulus vector gets multiplied with a transform matrix consisting of all the semantic pointers in the vocabulary. This yields a vector of similarities between the stimulus vector and the semantic pointers in the vocabulary. Each element in this vector will be thresholded and then multiplied with the transpose of the initial transform matrix. This converts the thresholded scalar similaritiesback into the semantic pointers and adds them up.\n",
    "\n",
    "Because all of the similarities are independent, we will use an ensemble array to represent these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with nengo.Network() as cleanup:\n",
    "    stimulus = nengo.Node(stimulus_pointer.v)\n",
    "    ea = nengo.networks.EnsembleArray(\n",
    "        n_neurons=50, n_ensembles=len(vocab.vectors), ens_dimensions=1)\n",
    "    output = nengo.Node(size_in=d)\n",
    "    \n",
    "    # This connection determins the similarities with the transform.\n",
    "    nengo.Connection(stimulus, ea.input, transform=vocab.vectors)\n",
    "    # Decoders from the ensemble array's ensembles do the thresholding.\n",
    "    ea.add_output('threshold', lambda x: 0. if x < 0.3 else 1.)\n",
    "    # This connections transforms the thresholded values back to semantic pointers.\n",
    "    nengo.Connection(ea.threshold, output, transform=vocab.vectors.T)\n",
    "    \n",
    "    p = nengo.Probe(output, synapse=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with nengo.Simulator(cleanup) as sim:\n",
    "    sim.run(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sim.trange(), spa.similarity(sim.data[p], vocab))\n",
    "plt.legend(vocab.keys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot better. The similarity of the output with $A$ is now close to 1 and there is much less noise. But we can improve this even further. Especially the similarity with the $B$ pointer is above 0 despite the thresholding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the tuning curves\n",
    "\n",
    "Let us take a look at the tuning curves of the neurons in one of the thresholding ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(*nengo.utils.ensemble.tuning_curves(ea.ensembles[0], sim))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Firing rate [Hz]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half of these neurons are tuned to fire more for smaller values. But these values are not really relevant as we are doing a thresholding at 0.3. Thus, we change all neurons to be tuned to fire more for larger values by setting all the encoders to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with nengo.Network() as cleanup:\n",
    "    stimulus = nengo.Node(stimulus_pointer.v)\n",
    "    ea = nengo.networks.EnsembleArray(\n",
    "        n_neurons=50, n_ensembles=len(vocab.vectors), ens_dimensions=1,\n",
    "        encoders=nengo.dists.Choice([[1]]))\n",
    "    output = nengo.Node(size_in=d)\n",
    "    \n",
    "    nengo.Connection(stimulus, ea.input, transform=vocab.vectors)\n",
    "    ea.add_output('threshold', lambda x: 0. if x < 0.3 else 1.)\n",
    "    nengo.Connection(ea.threshold, output, transform=vocab.vectors.T)\n",
    "    \n",
    "    p = nengo.Probe(output, synapse=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with nengo.Simulator(cleanup) as sim:\n",
    "    sim.run(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tuning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(*nengo.utils.ensemble.tuning_curves(ea.ensembles[0], sim))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Firing rate [Hz]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sim.trange(), spa.similarity(sim.data[p], vocab))\n",
    "plt.legend(vocab.keys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous plot the similarity with the $B$ and $C$ pointer is less noisy. The tuning curves are now all aligned in the correct direction, but they are still covering a lot of irrelevant area. Because all values below 0.3 should be 0, there is no need to have neurons tuned to this range. We want to shift all the intercepts to the range $(0.3, 1.0)$.\n",
    "\n",
    "But not only the range of intercepts can be important, but also the distribution of intercepts. Let us take a look at the thresholding function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-1, 1, 100)\n",
    "plt.plot(xs, xs >= 0.3)\n",
    "plt.ylim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is mostly constant except for that large jump at the threshold. The constant parts are easy to approximate and will not need a lot of neural resources, but the highly non-linear jump will require much more neural resources for an accurate representation.\n",
    "\n",
    "Thus, let us try to implement just this thresholding of a one-dimensional scalar in three ways: With a uniform distribution of intercepts (the default), all intercepts at 0.3 (where we have the non-linearity), and an exponential distribution. The last approach is in between the two extremes of a uniform distribution and placing all intercepts at 0.3. It will distribute most intercepts close to 0.3, but some intercepts will still be at larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "with nengo.Network() as threshold_net:\n",
    "    stimulus = nengo.Node(lambda t: t)\n",
    "    ens_uniform = nengo.Ensemble(\n",
    "        n_neurons=50, dimensions=1,\n",
    "        encoders=nengo.dists.Choice([[1]]), intercepts=nengo.dists.Uniform(threshold, 1.))\n",
    "    ens_fixed = nengo.Ensemble(\n",
    "        n_neurons=50, dimensions=1,\n",
    "        encoders=nengo.dists.Choice([[1]]), intercepts=nengo.dists.Choice([threshold]))\n",
    "    ens_exp = nengo.Ensemble(\n",
    "        n_neurons=50, dimensions=1,\n",
    "        encoders=nengo.dists.Choice([[1]]), intercepts=nengo.dists.Exponential(0.15, threshold, 1.))\n",
    "    \n",
    "    out_uniform = nengo.Node(size_in=1)\n",
    "    out_fixed = nengo.Node(size_in=1)\n",
    "    out_exp = nengo.Node(size_in=1)\n",
    "    \n",
    "    threshold_fn = lambda x: 0. if x < threshold else 1.\n",
    "    nengo.Connection(stimulus, ens_uniform)\n",
    "    nengo.Connection(stimulus, ens_fixed)\n",
    "    nengo.Connection(stimulus, ens_exp)\n",
    "    nengo.Connection(ens_uniform, out_uniform, function=threshold_fn)\n",
    "    nengo.Connection(ens_fixed, out_fixed, function=threshold_fn)\n",
    "    nengo.Connection(ens_exp, out_exp, function=threshold_fn)\n",
    "    \n",
    "    p_uniform = nengo.Probe(out_uniform, synapse=0.005)\n",
    "    p_fixed = nengo.Probe(out_fixed, synapse=0.005)\n",
    "    p_exp = nengo.Probe(out_exp, synapse=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with nengo.Simulator(threshold_net) as sim:\n",
    "    sim.run(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let as look at the tuning curves first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(*nengo.utils.ensemble.tuning_curves(ens_uniform, sim))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Firing rate [Hz]\")\n",
    "plt.title(\"Uniform intercepts\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(*nengo.utils.ensemble.tuning_curves(ens_fixed, sim))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Firing rate [Hz]\")\n",
    "plt.title(\"Fixed intercepts\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(*nengo.utils.ensemble.tuning_curves(ens_exp, sim))\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Firing rate [Hz]\")\n",
    "plt.title(\"Exponential intercept distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at how these three ensembles approximate the thresholding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sim.trange(), sim.data[p_uniform], label='Uniform intercepts')\n",
    "plt.plot(sim.trange(), sim.data[p_fixed], label='Fixed intercepts')\n",
    "plt.plot(sim.trange(), sim.data[p_exp], label='Exponential intercept dist.')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the fixed intercepts produce slightly higher decoded values close to the threshold, but the slope is lower than for uniform intercepts. The best approximation of the thresholding is done with the exponential intercept distribution. Here we get a quick rise to 1 at the threshold and a fairly constant representation of 1 for value sufficiently above the threshold. Thus, we will use this intercept distribution in our final cleanup network.\n",
    "\n",
    "Nengo provides the `ThresholdingEnsemble` preset to make it easier to assign intercepts according to that distribution as well as adjusting the encoders and evaluation points accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with nengo.Network() as cleanup:\n",
    "    stimulus = nengo.Node(stimulus_pointer.v)\n",
    "    with nengo.presets.ThresholdingEnsembles(0.3):\n",
    "        ea = nengo.networks.EnsembleArray(n_neurons=50, n_ensembles=len(vocab.vectors))\n",
    "    output = nengo.Node(size_in=d)\n",
    "    \n",
    "    nengo.Connection(stimulus, ea.input, transform=vocab.vectors)\n",
    "    ea.add_output('threshold', lambda x: 0. if x < 0.3 else 1.)\n",
    "    nengo.Connection(ea.threshold, output, transform=vocab.vectors.T)\n",
    "    \n",
    "    p = nengo.Probe(output, synapse=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with nengo.Simulator(cleanup) as sim:\n",
    "    sim.run(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sim.trange(), spa.similarity(sim.data[p], vocab))\n",
    "plt.legend(vocab.keys)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This gives the best cleanup with the least amount of noise.\n",
    "\n",
    "The take-away from this tutorial is that adjusting ensemble parameters in the right way can sometimes help in implementing functions more acurately in neurons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
